{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Peter-Herrmann/cv-vgg/blob/main/VGG_Experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FXw_bfvsW_b"
      },
      "source": [
        "### Classification with BHI Dataset and VGG-style network\n",
        "In this experiment you will set up a VGG-style network to classify histopathologic scans of breast tissue from the [BHI](https://www.kaggle.com/paultimothymooney/breast-histopathology-images) dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag4JjQZesW_c"
      },
      "source": [
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTDp7cfwsW_i"
      },
      "source": [
        "Here we use a Keras utility function to load the dataset.  I already organized the data into HDF5 files which are a good format for storing array data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsItJAHG1mFO"
      },
      "source": [
        "from tensorflow.keras.utils import get_file\n",
        "x_train_path = get_file('idc_train.h5','https://storage.googleapis.com/data401-datasets/idc_train.h5')\n",
        "x_test_path = get_file('idc_test.h5','https://storage.googleapis.com/data401-datasets/idc_test.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H55C1oJ1VnI"
      },
      "source": [
        "We read the data from the HDF5 files into Numpy arrays.\n",
        "\n",
        "I crop the images so they are all 48x48."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XijFf2dTXTJZ"
      },
      "source": [
        "import h5py as h5\n",
        "with h5.File(x_train_path,'r') as f:\n",
        "  x_train = f['X'][:,1:49,1:49]\n",
        "  y_train = f['y'][:]\n",
        "with h5.File(x_test_path,'r') as f:\n",
        "  x_test = f['X'][:,1:49,1:49]\n",
        "  y_test = f['y'][:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "php319eg7ncj"
      },
      "source": [
        "x_train.shape,y_train.shape,x_test.shape,y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxW1ADTQ1Yp7"
      },
      "source": [
        "Showing a few images from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD4AMCxWzMON"
      },
      "source": [
        "for i in range(5):\n",
        "  plt.imshow(np.squeeze(x_train[i]))\n",
        "  plt.title(y_train[i])\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing\n",
        "\n",
        "1. Convert the train and test images to floating point and divide by 255.\n",
        "2. Compute the average value of the entire training image set.\n",
        "3. Subtract the average value from the training and testing images.\n"
      ],
      "metadata": {
        "id": "HJQTAkh7oFGA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJfoF54nAqV9"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "QpTX8CjBsW_r"
      },
      "source": [
        "Build a VGG-style binary classifier model.  For example, your network could contain the following:\n",
        "1. 32 convolutional filters of size 3x3, zero padding, ReLU activation\n",
        "2. 2x2 max pooling with stride 2\n",
        "3. 64 filters\n",
        "4. max pool\n",
        "5. 128 filters\n",
        "5. max pool\n",
        "6. 256 filters\n",
        "7. max pool\n",
        "8. flatten\n",
        "9. Fully-connected layer with 128 outputs\n",
        "10. Final binary classification layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5HmyeD9sW_s"
      },
      "source": [
        "model = Sequential([\n",
        "    Input(x_train.shape[1:]),\n",
        "    #...\n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX2e5HSVsW_v"
      },
      "source": [
        "Set up the model to optimize the sparse categorical cross-entropy loss using Adam optimizer and learning rate of $.0003$.  Calculate accuracy metrics during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WpkJ4RDsW_w"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1idhItGsW_z"
      },
      "source": [
        "Now `fit` the model to the data using a batch size of 32 and 10% validation split over 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayvfekbHsW_z"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXqS38Pe1mSA"
      },
      "source": [
        "Plot loss and accuracy over the training run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SrRuhFusW_3"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['Training Loss','Validation Loss'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEuLVpDgsW_7"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.legend(['Training Accuracy','Validation Accuracy'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCOiAwyi1raG"
      },
      "source": [
        "Compute accuracy of the model on the training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBt9tQGLsW_-"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try a different setting to see if you can improve the test set accuracy at all.  Write about the results here."
      ],
      "metadata": {
        "id": "Be5YAWAkphZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "v7eMmkwvpwsf"
      }
    }
  ]
}